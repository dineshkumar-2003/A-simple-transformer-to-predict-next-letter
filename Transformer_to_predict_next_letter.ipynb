{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPTOBEJuzlxXPMCfgCJ4DPh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dineshkumar-2003/A-simple-transformer-to-predict-next-letter/blob/main/Transformer_to_predict_next_letter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnY44x6GLM7t"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=512):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)].to(x.device)\n",
        "\n",
        "class MultiHeadSelfAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.qkv_proj = nn.Linear(d_model, d_model * 3)\n",
        "        self.out_proj = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        qkv = self.qkv_proj(x)  # (B, T, 3 * C)\n",
        "        qkv = qkv.reshape(B, T, 3, self.num_heads, self.d_k).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]  # Each: (B, num_heads, T, d_k)\n",
        "\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / (self.d_k ** 0.5)  # (B, num_heads, T, T)\n",
        "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
        "        attn_output = attn_weights @ v  # (B, num_heads, T, d_k)\n",
        "\n",
        "        attn_output = attn_output.transpose(1, 2).reshape(B, T, C)\n",
        "        return self.out_proj(attn_output)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, ffn_hidden):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadSelfAttention(d_model, num_heads)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, ffn_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(ffn_hidden, d_model)\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ln1(x + self.attn(x))\n",
        "        x = self.ln2(x + self.ffn(x))\n",
        "        return x\n",
        "\n",
        "class MiniTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, num_heads=4, num_layers=2, ffn_hidden=512, max_len=128):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(d_model, num_heads, ffn_hidden) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.ln_f = nn.LayerNorm(d_model)\n",
        "        self.head = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_emb(x)\n",
        "        x = self.pos_enc(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        x = self.ln_f(x)\n",
        "        return self.head(x)\n",
        "\n",
        "# # Example usage\n",
        "# vocab_size = 5000\n",
        "# model = MiniTransformer(vocab_size)\n",
        "# x = torch.randint(0, vocab_size, (2, 32))  # (batch_size=2, seq_len=32)\n",
        "# logits = model(x)  # (2, 32, vocab_size)\n",
        "# print(logits.shape)  # should print torch.Size([2, 32, 5000])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hello world\"\n",
        "chars = sorted(list(set(text)))  # unique characters\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for ch,i in stoi.items() }\n",
        "data = torch.tensor([stoi[c] for c in text], dtype=torch.long)\n",
        "\n",
        "\n",
        "\n",
        "block_size = 4\n",
        "X = []\n",
        "Y = []\n",
        "for i in range(len(data) - block_size):\n",
        "    X.append(data[i:i+block_size])\n",
        "    Y.append(data[i+1:i+block_size+1])\n",
        "X = torch.stack(X)\n",
        "Y = torch.stack(Y)\n"
      ],
      "metadata": {
        "id": "Opha4PWvLpQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MiniTransformer(vocab_size=len(chars), d_model=32, num_heads=2, num_layers=2)\n"
      ],
      "metadata": {
        "id": "f-1OPwTGL60k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "for step in range(1000):\n",
        "    logits = model(X)  # (batch, seq_len, vocab)\n",
        "    loss = loss_fn(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step}, Loss {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9queHhvyL-5J",
        "outputId": "8f416d4e-c72a-422d-9ac9-8e2b3a7c6778"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0, Loss 2.1370\n",
            "Step 100, Loss 0.0383\n",
            "Step 200, Loss 0.0144\n",
            "Step 300, Loss 0.0076\n",
            "Step 400, Loss 0.0048\n",
            "Step 500, Loss 0.0033\n",
            "Step 600, Loss 0.0024\n",
            "Step 700, Loss 0.0019\n",
            "Step 800, Loss 0.0015\n",
            "Step 900, Loss 0.0012\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(start_seq, max_new_tokens=10):\n",
        "    model.eval()\n",
        "    input_ids = torch.tensor([stoi[c] for c in start_seq], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        input_chunk = input_ids[:, -block_size:]\n",
        "        logits = model(input_chunk)\n",
        "        next_id = torch.argmax(logits[:, -1, :], dim=-1)\n",
        "        input_ids = torch.cat([input_ids, next_id.unsqueeze(0)], dim=1)\n",
        "\n",
        "    return ''.join([itos[i] for i in input_ids[0].tolist()])\n"
      ],
      "metadata": {
        "id": "jpVHn2S3MeaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate(\"w\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WDh0vqBMDq3",
        "outputId": "da64430b-fe92-4af3-df21-e769715a88a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "worldrldrld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v9XZrOGrMITW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}